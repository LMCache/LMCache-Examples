{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb0oCMPmprFk"
      },
      "source": [
        "# **üß™ Running LMCache with vLLM on Google Colab**\n",
        "\n",
        "This Colab notebook demonstrates how to run LMCache with the vLLM inference engine. We use Meta's Llama-3.1-8B-Instruct model as an example.\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <a href=\"https://lmcache.ai/\" style=\"display:inline-block; margin:0 1em; text-decoration:none;\">\n",
        "    <img\n",
        "      src=\"https://raw.githubusercontent.com/LMCache/LMCache/2e4c7b95a0784babd6d61313724a801614898e1e/docs/source/assets/lmcache-logo_crop.png\"\n",
        "      alt=\"LMCache logo\"\n",
        "      width=\"170\"\n",
        "      style=\"vertical-align:middle; border:none;\"\n",
        "    />\n",
        "  </a>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\" style=\"margin-top:.5em;\">\n",
        "  <a href=\"https://join.slack.com/t/lmcacheworkspace/shared_invite/zt-2viziwhue-5Amprc9k5hcIdXT7XevTaQ\" style=\"text-decoration:none;\">\n",
        "    <img\n",
        "      src=\"https://upload.wikimedia.org/wikipedia/commons/b/b9/Slack_Technologies_Logo.svg\"\n",
        "      alt=\"Slack logo\"\n",
        "      width=\"125\"\n",
        "      style=\"vertical-align:middle; margin-right: 0.5em;\"\n",
        "    />\n",
        "  </a>\n",
        "</p>\n",
        "\n",
        "<!-- GitHub line -->\n",
        "<p align=\"center\">\n",
        "  <em><b>Join Slack if you need help + ‚≠ê Star us on <a href=\"https://github.com/LMCache/LMCache\" style=\"text-decoration:none;\">GitHub</a> ‚≠ê</b></em>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-b468rSprH9"
      },
      "source": [
        "## ‚öôÔ∏è Configure Colab Runtime\n",
        "\n",
        "To enable GPU acceleration on Google Colab:\n",
        "\n",
        "1. Click the **Runtime** menu in the top toolbar.\n",
        "2. Select **Change runtime type**.\n",
        "3. In the **Hardware accelerator** dropdown, choose **GPU** (preferably **A100 GPU**, as LMCache currently does not support the T4 GPU).\n",
        "4. Click **Save**.\n",
        "\n",
        "> üìå You can confirm GPU access by running the following cell:\n",
        ">\n",
        "> ```python\n",
        "> !nvidia-smi\n",
        "> ```\n",
        "\n",
        "---\n",
        "\n",
        "## üîê Set Up Hugging Face Credentials\n",
        "\n",
        "Since this demo uses **Meta‚Äôs Llama-3.1-8B-Instruct model**, you‚Äôll need to set up your Hugging Face account and request access:\n",
        "\n",
        "1. **Sign up** for a free account at [https://huggingface.co/join](https://huggingface.co/join).\n",
        "2. **Request access** to the LLaMA 3 model here:  \n",
        "   üëâ [Llama-3.1-8B-Instruct Model Card](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)  \n",
        "3. Once approved, **create a Hugging Face access token**:  \n",
        "   üëâ [Token Settings Page](https://huggingface.co/settings/tokens)  \n",
        "4. Click on the sidebar (left panel) ‚Üí **\"Secrets\"** tab\n",
        "5. Click **‚Äú+ Add new secret‚Äù**\n",
        "*   Name: HF_TOKEN\n",
        "*   Value: (paste your Hugging Face token)\n",
        "\n",
        "> üí° Your token will be used to authenticate and download the model via the Hugging Face.\n",
        "\n",
        "\n",
        "> Access your secret keys in Python via:\n",
        ">\n",
        "> ```python\n",
        "> from google.colab import userdata\n",
        "> hf_token = userdata.get(\"HF_TOKEN\")\n",
        "> ```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgezu8BYprKA"
      },
      "source": [
        "## Install vLLM v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISdxobrrprMX"
      },
      "source": [
        "üì¶ Install uv (a fast Python package manager)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUUUcOBzuCrp"
      },
      "outputs": [],
      "source": [
        "!curl -LsSf https://astral.sh/uv/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8Cgqn3oprOb"
      },
      "source": [
        "üì¶ Install the latest nightly version of vLLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Hayg5fBuUU3"
      },
      "outputs": [],
      "source": [
        "!uv pip install -U vllm --pre --extra-index-url https://wheels.vllm.ai/nightly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PWZxN08priB"
      },
      "source": [
        "üì¶ Install LMCache from source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caQV2YgtubS5"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/LMCache/LMCache.git\n",
        "%cd LMCache\n",
        "!uv pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGkH3zz_prkT"
      },
      "source": [
        "## Run Inference **without** LMCache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u39NiMODvBRZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from vllm import LLM, SamplingParams\n",
        "from vllm.config import KVTransferConfig\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set token chunk size\n",
        "os.environ[\"LMCACHE_CHUNK_SIZE\"] = \"256\"\n",
        "\n",
        "# Enable CPU offloading backend\n",
        "os.environ[\"LMCACHE_LOCAL_CPU\"] = \"True\"\n",
        "\n",
        "# Set CPU memory limit (in GB)\n",
        "os.environ[\"LMCACHE_MAX_LOCAL_CPU_SIZE\"] = \"5.0\"\n",
        "\n",
        "# Set Hugging Face access token\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "os.environ[\"HF_TOKEN\"] = hf_token\n",
        "\n",
        "# Input\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/long_document.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    shared_prompt = f.read()\n",
        "prompts = [shared_prompt + \"\\n\\n\" + \"When did the Roman Empire begin and when did it fall?\"]\n",
        "\n",
        "# Set sampling parameters\n",
        "sampling_params = SamplingParams(temperature=0, top_p=0.95, max_tokens=10)\n",
        "\n",
        "print(\"üîÅ Running generation WITHOUT LMCache...\")\n",
        "\n",
        "# Initialize vLLM without LMCache integration\n",
        "llm_no_lmcache = LLM(\n",
        "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "    max_model_len=8000,\n",
        "    dtype=\"float16\",\n",
        "    enable_prefix_caching=False,\n",
        "    gpu_memory_utilization=0.8,\n",
        ")\n",
        "\n",
        "# First run (cold cache, no reuse)\n",
        "time_start_1 = time.time()\n",
        "outputs = llm_no_lmcache.generate(prompts, sampling_params)\n",
        "for output in outputs:\n",
        "    print(\"Generated text (no LMCache, 1st run):\", repr(output.outputs[0].text))\n",
        "time_end_1 = time.time()\n",
        "\n",
        "# Second run (still no cache reuse)\n",
        "time_start_2 = time.time()\n",
        "outputs = llm_no_lmcache.generate(prompts, sampling_params)\n",
        "for output in outputs:\n",
        "    print(\"Generated text (no LMCache, 2nd run):\", repr(output.outputs[0].text))\n",
        "time_end_2 = time.time()\n",
        "\n",
        "print(f\"‚ùå No LMCache - 1st run duration: {time_end_1 - time_start_1:.2f} seconds\")\n",
        "print(f\"‚ùå No LMCache - 2nd run duration: {time_end_2 - time_start_2:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpk6CwAYprmJ"
      },
      "source": [
        "## Run Inference **with** LMCache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-0Z_d80NwCe2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from vllm import LLM, SamplingParams\n",
        "from vllm.config import KVTransferConfig\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set token chunk size\n",
        "os.environ[\"LMCACHE_CHUNK_SIZE\"] = \"256\"\n",
        "\n",
        "# Enable CPU offloading backend\n",
        "os.environ[\"LMCACHE_LOCAL_CPU\"] = \"True\"\n",
        "\n",
        "# Set CPU memory limit (in GB)\n",
        "os.environ[\"LMCACHE_MAX_LOCAL_CPU_SIZE\"] = \"5.0\"\n",
        "\n",
        "# Set Hugging Face access token\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "os.environ[\"HF_TOKEN\"] = hf_token\n",
        "\n",
        "# Input\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/long_document.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    shared_prompt = f.read()\n",
        "prompts = [shared_prompt + \"\\n\\n\" + \"When did the Roman Empire begin and when did it fall?\"]\n",
        "\n",
        "# Set sampling parameters\n",
        "sampling_params = SamplingParams(temperature=0, top_p=0.95, max_tokens=10)\n",
        "\n",
        "print(\"üîÅ Running generation WITHOUT LMCache...\")\n",
        "\n",
        "# Set up LMCache connector for KV cache transfer\n",
        "ktc = KVTransferConfig(\n",
        "    kv_connector=\"LMCacheConnectorV1\",  # MUST match registered name\n",
        "    kv_role=\"kv_both\",  # both read and write\n",
        ")\n",
        "\n",
        "# Initialize vLLM with LMCache integration\n",
        "llm = LLM(\n",
        "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "    kv_transfer_config=ktc,\n",
        "    max_model_len=8000,\n",
        "    dtype=\"float16\",\n",
        "    gpu_memory_utilization=0.8,\n",
        ")\n",
        "\n",
        "# First run (LMCache will store KV cache)\n",
        "time_start_1 = time.time()\n",
        "outputs = llm.generate(prompts, sampling_params)\n",
        "for output in outputs:\n",
        "    print(\"Generated text (LMCache, 1st run):\", repr(output.outputs[0].text))\n",
        "time_end_1 = time.time()\n",
        "\n",
        "# Second run (LMCache reuses KV cache)\n",
        "time_start_2 = time.time()\n",
        "outputs = llm.generate(prompts, sampling_params)\n",
        "for output in outputs:\n",
        "    print(\"Generated text (LMCache, 2nd run):\", repr(output.outputs[0].text))\n",
        "time_end_2 = time.time()\n",
        "\n",
        "print(f\"‚úÖ LMCache - 1st run duration: {time_end_1 - time_start_1:.2f} seconds\")\n",
        "print(f\"‚úÖ LMCache - 2nd run duration: {time_end_2 - time_start_2:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs2Lo9mCproP"
      },
      "source": [
        "Clean Up LMCache Engine:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyRw9PHrwVC4"
      },
      "outputs": [],
      "source": [
        "from lmcache.v1.cache_engine import LMCacheEngineBuilder\n",
        "from lmcache.integration.vllm.utils import ENGINE_NAME\n",
        "\n",
        "# Properly clean up the LMCache backend\n",
        "LMCacheEngineBuilder.destroy(ENGINE_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPyU7KPUprqi"
      },
      "source": [
        "> üìå During inference, LMCache will automatically handle storing and managing KV cache in CPU memory. You can monitor this through the logs, which will show messages like:\n",
        ">\n",
        "> ```python\n",
        "> LMCache INFO: Storing KV cache for 6006 out of 6006 tokens for request 0\n",
        "> ```\n",
        "\n",
        "This means KV cache was successfully offloaded to CPU memory.\n",
        "\n",
        "Note\n",
        "\n",
        "\n",
        "*   Adjust gpu_memory_utilization based on your GPU's available memory\n",
        "*   The CPU offloading buffer size can be adjusted through `LMCACHE_MAX_LOCAL_CPU_SIZE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub9ro3S5xLMx"
      },
      "source": [
        "## What happens in real life without LMCache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgd5jRXjxYaW"
      },
      "source": [
        "### Without LMCache: old cache gets evicted as new queries come in.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "B9RwINCVxVuS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f7KKIzIxhwS"
      },
      "source": [
        "### With LMCache: old cache is offloaded and reused."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ReADIGy4xh3W"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "zgezu8BYprKA",
        "bpk6CwAYprmJ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
